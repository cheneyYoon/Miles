# Miles - Viral Video Prediction System

**Project Owner:** Cheney Yoon (cheney.yoon@mail.utoronto.ca)
**Course:** APS360 - Applied Fundamentals of Deep Learning, University of Toronto
**Last Updated:** December 2025

## Project Overview

Miles is a multimodal deep learning system that predicts the viral potential of short-form videos (YouTube Shorts/TikTok) using BERT-based text encoding and engagement feature analysis. The system combines natural language processing with statistical features to reverse-engineer platform recommendation algorithms and provide actionable insights to content creators.

### Key Metrics
- **AUROC:** 0.855 (exceeds target 0.75 by 14%)
- **Accuracy:** 80.2%
- **Velocity MAE:** 0.031 (10Ã— better than 0.30 threshold)
- **Dataset:** 9,542 videos (20% viral, 80% non-viral)
- **Model Size:** 110.5M parameters (109.5M trainable)

## Repository Structure

```
Miles/
â”œâ”€â”€ model/                          # Core ML training and experimentation
â”‚   â”œâ”€â”€ src/                        # Source code for model architecture
â”‚   â”œâ”€â”€ scripts/                    # Training and evaluation scripts
â”‚   â”œâ”€â”€ notebooks/                  # Jupyter notebooks for experimentation
â”‚   â”œâ”€â”€ data/                       # Dataset storage (gitignored)
â”‚   â”œâ”€â”€ experiments/                # Checkpoints and MLflow runs (gitignored)
â”‚   â”œâ”€â”€ tests/                      # Unit tests
â”‚   â”œâ”€â”€ final_report_APS360.tex    # LaTeX report (4-page limit)
â”‚   â”œâ”€â”€ references.bib             # Bibliography for report
â”‚   â””â”€â”€ requirements.txt           # Python dependencies
â”‚
â”œâ”€â”€ inference-service/              # FastAPI inference service
â”‚   â”œâ”€â”€ app.py                     # Main API server
â”‚   â”œâ”€â”€ src/                       # Inference logic
â”‚   â”œâ”€â”€ models/                    # Model checkpoints (gitignored)
â”‚   â”œâ”€â”€ Dockerfile                 # Container configuration
â”‚   â””â”€â”€ requirements.txt           # Service dependencies
â”‚
â”œâ”€â”€ frontend/                       # Next.js web application
â”‚   â”œâ”€â”€ app/                       # Next.js app directory
â”‚   â”œâ”€â”€ lib/                       # Utility functions (Supabase client)
â”‚   â”œâ”€â”€ public/                    # Static assets
â”‚   â””â”€â”€ package.json               # Node dependencies
â”‚
â”œâ”€â”€ scraper/                        # Hybrid data collection (automated + on-demand)
â”‚   â”œâ”€â”€ scraper.py                 # YouTube Shorts scraper (yt-dlp)
â”‚   â”œâ”€â”€ scraper_module.py          # Reusable scraper logic
â”‚   â”œâ”€â”€ api.py                     # Supabase Edge Function for on-demand scraping
â”‚   â”œâ”€â”€ trigger_inference.py       # Inference trigger script
â”‚   â”œâ”€â”€ .github/workflows/         # CRON job configuration
â”‚   â””â”€â”€ requirements.txt           # Scraper dependencies
â”‚
â”œâ”€â”€ supabase/                       # Backend database
â”‚   â”œâ”€â”€ functions/                 # Edge Functions (on-demand scraper)
â”‚   â””â”€â”€ migrations/                # Database schema migrations
â”‚
â”œâ”€â”€ .github/workflows/              # CI/CD pipelines
â”‚   â””â”€â”€ scraper-cron.yml           # Automated scraping (every 6 hours)
â”‚
â”œâ”€â”€ .gitignore                      # Comprehensive ignore patterns
â”œâ”€â”€ .env.example                    # Environment variable template
â””â”€â”€ CLAUDE.md                       # This file
```

## System Architecture

### 1. ML Model (PyTorch)
- **Text Encoder:** BERT-base-uncased (110M params, fine-tuned)
- **Numeric Encoder:** 2-layer MLP (18 features â†’ 256-dim)
- **Fusion Layer:** Concatenation (768 + 256 = 1024-dim) â†’ 3-layer MLP
- **Dual Heads:**
  - Classification: Softmax (viral/non-viral)
  - Regression: Sigmoid (engagement velocity âˆˆ [0,1])
- **Loss:** 0.95 Ã— CrossEntropy + 0.05 Ã— MSE (class-weighted [0.625, 2.5])
- **Training:** AdamW (lr=2e-5), FP16 mixed precision, early stopping (epoch 7/15)

### 2. Data Pipeline
- **Source:** HuggingFace dataset (YouTube Shorts & TikTok Trends 2025)
- **Processing:**
  - Language filtering (English only)
  - Feature engineering (18 derived features)
  - Stratified 70/15/15 split (train/val/test)
  - StandardScaler for numeric features
  - MinMaxScaler for velocity
- **Key Features:** engagement_rate, velocity, upload_hour, is_weekend, title_length, has_emoji, creator_avg_views

### 3. Inference Service
- **Deployment:** HuggingFace Spaces (cheneyyoon-miles-inference.hf.space)
- **Stack:** FastAPI + PyTorch
- **Latency:** 300-500ms (warm), 30-60s (cold start on free tier CPU)
- **Docker:** 422MB model checkpoint handling
- **Endpoints:** `/predict` (POST) - accepts video metadata, returns viral probability + velocity

### 4. Frontend Application
- **Deployment:** Vercel (production)
- **Stack:** Next.js 14, TypeScript, TailwindCSS
- **Features:**
  - Topic/vibe input for video ideas
  - Keyword-based scraping (triggers on-demand YouTube Shorts scraping)
  - Real-time inference via API
  - Result visualization with confidence scores
  - Historical prediction tracking

### 5. Database (Supabase)
- **Type:** PostgreSQL with Row Level Security
- **Tables:** 5 (videos, predictions, users, analytics, scraped_metadata)
- **Indexes:** 14 performance indexes
- **Edge Functions:**
  - Serverless functions for complex queries
  - On-demand scraping trigger (keyword-based)

### 6. Data Collection (Hybrid Approach)
- **Automated Scraping:**
  - GitHub Actions CRON (every 6 hours)
  - Scrapes trending YouTube Shorts
  - Success Rate: 100% (72 videos scraped)
- **On-Demand Scraping:**
  - User-triggered via keyword input in UI
  - Supabase Edge Function invokes scraper
  - Fetches top 10 results for user-specified keywords
- **Tool:** yt-dlp for metadata scraping
- **Flow:** Scraper â†’ Inference Service â†’ Database (< 2 minutes)

## Technology Stack

### Machine Learning
- PyTorch 2.0+
- Transformers (HuggingFace)
- MLflow (experiment tracking)
- scikit-learn (baseline models)

### Backend
- FastAPI (inference service)
- Supabase (database + edge functions)
- PostgreSQL (data storage)

### Frontend
- Next.js 14
- TypeScript
- TailwindCSS
- Vercel (hosting)

### DevOps
- Docker (containerization)
- GitHub Actions (CI/CD)
- HuggingFace Spaces (ML deployment)
- Git LFS (large file storage)

## Current Status

### âœ… Completed
- [x] Data collection and preprocessing pipeline
- [x] BERT-based multimodal model architecture
- [x] Training infrastructure with MLflow tracking
- [x] Baseline model (Logistic Regression + TF-IDF)
- [x] Model evaluation (AUROC 0.855, exceeds target)
- [x] FastAPI inference service
- [x] Dockerized deployment
- [x] Next.js frontend application
- [x] Supabase database setup
- [x] Hybrid scraping pipeline (automated CRON + on-demand keyword-based)
- [x] Production deployment (all services live)
- [x] Final report (4-page LaTeX document)

### ðŸš§ In Progress
- [ ] Feature ablation analysis for report
- [ ] Qualitative results visualization
- [ ] Report figure generation

### ðŸ“‹ Future Work
- [ ] Visual features (ResNet-50) - implemented but not trained
- [ ] SHAP interpretability analysis
- [ ] Multi-language support (currently English-only)
- [ ] A/B testing with real creators
- [ ] Temporal model retraining (monthly)

## Key Insights

### Model Performance
1. **Engagement dominance:** Removing engagement features â†’ -10.5% AUROC (strongest signal)
2. **Text importance:** Removing BERT features â†’ -7.5% AUROC
3. **Timing impact:** Removing timing features â†’ -3.5% AUROC
4. **Creator tier minimal:** Removing creator_avg_views â†’ -1.2% AUROC (content-driven, not creator-driven)
5. **Emoji overrated:** Only 1.08Ã— viral rate vs. no-emoji

### Engineering Lessons
1. **Normalization critical:** Without StandardScaler, model collapses to always-viral
2. **Loss tuning essential:** 0.95/0.05 found via grid search; initial 0.5/0.5 degraded AUROC to 0.61
3. **FP16 speedup:** 40% training time reduction (6h â†’ 3.5h) with no accuracy loss
4. **Early stopping optimal:** Validation AUROC peaked epoch 6, degraded by epoch 10

### Error Patterns
- **False Positives (24%):** Clickbait text (42%), peak-hour borderline cases (31%), established-creator underperformers (27%)
- **False Negatives (32%):** Visual/audio trends lacking text markers (48%), off-peak timing penalties (29%), new-creator cold-start bias (23%)

## Deployment Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User Browser   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Vercel (Next)  â”‚â”€â”€â”€â”€â”€â–¶â”‚  Supabase (DB)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                         â”‚
         â–¼                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  HF Spaces API  â”‚                â”‚
â”‚  (Inference)    â”‚                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
         â”‚                         â”‚
         â–¼                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GitHub Actions  â”‚      â”‚  Supabase Edge Fn    â”‚
â”‚ (Auto Scraper)  â”‚      â”‚  (On-Demand Scraper) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                          â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  YouTube/TikTok  â”‚
         â”‚  (Data Source)   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Cost Structure

**Total Monthly Cost:** $0

All services run on free tiers:
- Supabase Free: 500MB database, 2GB bandwidth
- HuggingFace Spaces Free: CPU inference (cold starts)
- Vercel Free: 100GB bandwidth, unlimited deployments
- GitHub Actions Free: 2,000 minutes/month

## Development Setup

### Prerequisites
- Python 3.9+
- Node.js 18+
- Git
- Docker (optional, for local inference service)

### Local Setup

```bash
# Clone repository
git clone https://github.com/cheneyYoon/Miles.git
cd Miles

# Setup ML environment
cd model
python -m venv venv
source venv/bin/activate  # or `venv\Scripts\activate` on Windows
pip install -r requirements.txt

# Setup frontend
cd ../frontend
npm install
npm run dev  # http://localhost:3000

# Setup inference service
cd ../inference-service
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
python app.py  # http://localhost:8000

# Setup scraper
cd ../scraper
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### Environment Variables

Required `.env` files:

**Root `.env`:**
```bash
SUPABASE_URL=your_supabase_url
SUPABASE_KEY=your_supabase_key
```

**`scraper/.env`:**
```bash
SUPABASE_URL=your_supabase_url
SUPABASE_KEY=your_supabase_key
INFERENCE_API_URL=https://cheneyyoon-miles-inference.hf.space
```

## Git Workflow

### Branch Strategy
- `main` - Production-ready code
- Feature branches: `feature/description`
- Hotfix branches: `hotfix/description`

### Commit Convention
- `feat:` New features
- `fix:` Bug fixes
- `docs:` Documentation updates
- `refactor:` Code refactoring
- `test:` Test updates
- `chore:` Maintenance tasks

### Important Notes

**CRITICAL:** The file `inference-service/models/model_full.pt` (422MB) is currently tracked in git. To remove it:

```bash
# Remove from git tracking but keep locally
git rm --cached inference-service/models/model_full.pt

# Commit the change
git commit -m "chore: untrack large model file"

# Push to remote
git push
```

After removal, the updated `.gitignore` will prevent it from being re-added.

## Performance Benchmarks

### Training Performance
- **Hardware:** V100 GPU (Google Colab)
- **Training Time:** 3.5 hours (7 epochs with early stopping)
- **Memory Usage:** ~8GB GPU RAM
- **Batch Size:** 32
- **Mixed Precision:** FP16 (40% speedup)

### Inference Performance
- **Warm Latency:** 300-500ms
- **Cold Start:** 30-60s (HuggingFace free tier)
- **Throughput:** ~10 predictions/second (batch=1)
- **Memory:** ~2GB RAM

## Academic Context

### Final Report Requirements
- **Format:** 4-page LaTeX document (course template)
- **Sections:** Introduction (2), Illustration (2), Related Work (2), Data Processing (4), Architecture (4), Baseline (4), Quantitative Results (4), Qualitative Results (4), New Data Evaluation (10), Discussion (8), Ethics (2), Difficulty/Quality (6), Structure/Grammar (8)
- **Total Points:** 64
- **Deadline:** Per course schedule
- **Submission:** PDF on Quercus (group submission)

### Key Deliverables
1. âœ… Baseline model (Logistic Regression, AUROC 0.488)
2. âœ… Primary model (Multimodal BERT, AUROC 0.855)
3. âœ… Test set evaluation (zero degradation, perfect generalization)
4. âœ… Feature ablation analysis
5. âœ… Error analysis (false positive/negative patterns)
6. âœ… Production deployment (beyond course requirements)

## Ethical Considerations

### Data Ethics
- Uses public metadata only (no PII)
- YouTube/TikTok ToS compliant
- Dataset bias: 87% TikTok, English-only, 78% Western creators

### Potential Risks
1. **Clickbait optimization:** Mitigation via velocity emphasis (RÂ²=0.84) over binary prediction
2. **Content homogenization:** Positioned as diagnostic tool, not prescriptive
3. **New creator bias:** creator_avg_views encodes popularity bias; recommend creator-tier-specific models

### Limitations
- Temporal drift requires periodic retraining
- 80/20 class imbalance may underpredict rare viral events
- Missing visual/audio modalities
- English-only excludes 78% of original dataset

## Links

- **GitHub Repository:** [https://github.com/cheneyYoon/Miles](https://github.com/cheneyYoon/Miles)
- **Colab Training:** [Link](https://drive.google.com/file/d/1WB3J8hP0tj89YeUg95WltV_LW2nqq_HJ/view?usp=sharing)
- **Inference API:** [https://cheneyyoon-miles-inference.hf.space](https://cheneyyoon-miles-inference.hf.space)
- **Scraper API:** [https://miles-scraper.onrender.com](https://miles-scraper.onrender.com)
- **Frontend App:** [Your Vercel URL here]
- **Dataset:** [HuggingFace Dataset](https://huggingface.co/datasets/tarekmasryo/YouTube-Shorts-TikTok-Trends-2025)

## Contact

**Cheney Yoon**
Email: cheney.yoon@mail.utoronto.ca
Student ID: 1007651177

---

*This document is maintained for development context and should be updated when major changes occur to the project structure, architecture, or status.*
