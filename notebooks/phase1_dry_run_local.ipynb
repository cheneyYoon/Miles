{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1 Training - LOCAL DRY RUN TEST\n",
    "\n",
    "**Purpose:** Validate the entire training pipeline on local machine with CPU and minimal data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Setup Check\n",
    "\n",
    "Make sure you're in the correct directory and have dependencies installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /Users/cheneyyoon/Desktop/U of T/APS360/Miles/notebooks\n",
      "Project root: /Users/cheneyyoon/Desktop/U of T/APS360/Miles\n",
      "\n",
      "Changed to: /Users/cheneyyoon/Desktop/U of T/APS360/Miles\n",
      "\n",
      "‚úÖ Directory setup correct!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Get project root (assuming notebook is in notebooks/ folder)\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir.parent if notebook_dir.name == 'notebooks' else notebook_dir\n",
    "\n",
    "print(f\"Current directory: {Path.cwd()}\")\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "# Change to project root\n",
    "os.chdir(project_root)\n",
    "print(f\"\\nChanged to: {os.getcwd()}\")\n",
    "\n",
    "# Verify we're in the right place\n",
    "assert (Path.cwd() / 'src').exists(), \"‚ùå 'src' folder not found! Are you in the project root?\"\n",
    "assert (Path.cwd() / 'requirements.txt').exists(), \"‚ùå 'requirements.txt' not found!\"\n",
    "\n",
    "print(\"\\n‚úÖ Directory setup correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import All Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.4 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/cheneyyoon/Desktop/U of T/APS360/Miles/venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/cheneyyoon/Desktop/U of T/APS360/Miles/venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/cheneyyoon/Desktop/U of T/APS360/Miles/venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 758, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/cheneyyoon/Desktop/U of T/APS360/Miles/venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 639, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 1985, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/cheneyyoon/Desktop/U of T/APS360/Miles/venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n",
      "    await self.dispatch_shell(msg, subshell_id=subshell_id)\n",
      "  File \"/Users/cheneyyoon/Desktop/U of T/APS360/Miles/venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/cheneyyoon/Desktop/U of T/APS360/Miles/venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/cheneyyoon/Desktop/U of T/APS360/Miles/venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/cheneyyoon/Desktop/U of T/APS360/Miles/venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/cheneyyoon/Desktop/U of T/APS360/Miles/venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/cheneyyoon/Desktop/U of T/APS360/Miles/venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/cheneyyoon/Desktop/U of T/APS360/Miles/venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/cheneyyoon/Desktop/U of T/APS360/Miles/venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/cheneyyoon/Desktop/U of T/APS360/Miles/venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/cheneyyoon/Desktop/U of T/APS360/Miles/venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/cheneyyoon/Desktop/U of T/APS360/Miles/venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/yg/1222qhpn71sclw0r8zspmz2h0000gn/T/ipykernel_16313/3927687266.py\", line 7, in <module>\n",
      "    import torch\n",
      "  File \"/Users/cheneyyoon/Desktop/U of T/APS360/Miles/venv/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/cheneyyoon/Desktop/U of T/APS360/Miles/venv/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/cheneyyoon/Desktop/U of T/APS360/Miles/venv/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/cheneyyoon/Desktop/U of T/APS360/Miles/venv/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/cheneyyoon/Desktop/U of T/APS360/Miles/venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing imports...\n",
      "PyTorch version: 2.2.2\n",
      "Pandas version: 2.3.3\n",
      "NumPy version: 2.3.4\n",
      "‚úÖ Dataset adapter imported\n",
      "‚úÖ Data modules imported\n",
      "‚úÖ Model modules imported\n",
      "‚úÖ Training modules imported\n",
      "\n",
      "‚úÖ ALL IMPORTS SUCCESSFUL!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd() / 'src'))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"Testing imports...\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "\n",
    "try:\n",
    "    # Import dataset adapter\n",
    "    from data.dataset_adapter import (\n",
    "        prepare_dataset_for_training,\n",
    "        get_available_scalar_features,\n",
    "        get_dataset_summary\n",
    "    )\n",
    "    print(\"‚úÖ Dataset adapter imported\")\n",
    "    \n",
    "    from data.download import download_dataset\n",
    "    from data.preprocessing import preprocess_dataset\n",
    "    from data.dataset import create_train_val_test_split, create_data_loaders\n",
    "    print(\"‚úÖ Data modules imported\")\n",
    "\n",
    "    from models.baseline import BaselineModel\n",
    "    from models.fusion_model import MultimodalViralityPredictor\n",
    "    print(\"‚úÖ Model modules imported\")\n",
    "\n",
    "    from training.utils import load_config, set_seed, get_device, save_checkpoint\n",
    "    from training.evaluate import evaluate_model, print_evaluation_report\n",
    "    from training.train import train_model\n",
    "    print(\"‚úÖ Training modules imported\")\n",
    "    \n",
    "    print(\"\\n‚úÖ ALL IMPORTS SUCCESSFUL!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå IMPORT ERROR: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Make sure you installed dependencies: pip install -r requirements.txt\")\n",
    "    print(\"2. Check that all __init__.py files exist in src/ folders\")\n",
    "    print(\"3. Verify file structure matches implementation plan\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Tiny Dataset Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TINY dataset from: data/raw/youtube_shorts_tiktok_trends_2025.csv\n",
      "(Only 100 rows for dry run)\n",
      "‚úÖ Loaded 100 rows\n",
      "\n",
      "Shape: (100, 58)\n",
      "\n",
      "First 10 columns: ['platform', 'country', 'region', 'language', 'category', 'hashtag', 'title_keywords', 'author_handle', 'sound_type', 'music_track']\n",
      "\n",
      "First 3 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>platform</th>\n",
       "      <th>country</th>\n",
       "      <th>region</th>\n",
       "      <th>language</th>\n",
       "      <th>category</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>title_keywords</th>\n",
       "      <th>author_handle</th>\n",
       "      <th>sound_type</th>\n",
       "      <th>music_track</th>\n",
       "      <th>...</th>\n",
       "      <th>traffic_source</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>row_id</th>\n",
       "      <th>engagement_total</th>\n",
       "      <th>like_rate</th>\n",
       "      <th>dislike_rate</th>\n",
       "      <th>engagement_per_1k</th>\n",
       "      <th>engagement_like_rate</th>\n",
       "      <th>engagement_comment_rate</th>\n",
       "      <th>engagement_share_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TikTok</td>\n",
       "      <td>Jp</td>\n",
       "      <td>Asia</td>\n",
       "      <td>ja</td>\n",
       "      <td>Gaming</td>\n",
       "      <td>#Lifestyle</td>\n",
       "      <td>Night Routine ‚Äî College</td>\n",
       "      <td>NextVision</td>\n",
       "      <td>trending</td>\n",
       "      <td>8bit loop</td>\n",
       "      <td>...</td>\n",
       "      <td>External</td>\n",
       "      <td>1</td>\n",
       "      <td>2e681528d17a1fe1986857942536ec27</td>\n",
       "      <td>30317</td>\n",
       "      <td>0.086159</td>\n",
       "      <td>0.004004</td>\n",
       "      <td>120.069</td>\n",
       "      <td>0.086159</td>\n",
       "      <td>0.012555</td>\n",
       "      <td>0.007830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TikTok</td>\n",
       "      <td>Se</td>\n",
       "      <td>Europe</td>\n",
       "      <td>sv</td>\n",
       "      <td>Food</td>\n",
       "      <td>#Sports</td>\n",
       "      <td>Morning Routine ‚Äî College</td>\n",
       "      <td>DailyVlogsDiego</td>\n",
       "      <td>trending</td>\n",
       "      <td>Street vibe</td>\n",
       "      <td>...</td>\n",
       "      <td>Search</td>\n",
       "      <td>0</td>\n",
       "      <td>2e35fa0b2978b9cae635839c1d4e9e74</td>\n",
       "      <td>30577</td>\n",
       "      <td>0.085298</td>\n",
       "      <td>0.002421</td>\n",
       "      <td>113.005</td>\n",
       "      <td>0.085298</td>\n",
       "      <td>0.007850</td>\n",
       "      <td>0.007791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TikTok</td>\n",
       "      <td>Za</td>\n",
       "      <td>Africa</td>\n",
       "      <td>en</td>\n",
       "      <td>Art</td>\n",
       "      <td>#Workout</td>\n",
       "      <td>Night Routine ‚Äî College</td>\n",
       "      <td>BeyondHub</td>\n",
       "      <td>licensed</td>\n",
       "      <td>Gallery pad</td>\n",
       "      <td>...</td>\n",
       "      <td>External</td>\n",
       "      <td>1</td>\n",
       "      <td>0d88a011235a82244995ef52961f9502</td>\n",
       "      <td>503</td>\n",
       "      <td>0.049154</td>\n",
       "      <td>0.001625</td>\n",
       "      <td>68.111</td>\n",
       "      <td>0.049154</td>\n",
       "      <td>0.004469</td>\n",
       "      <td>0.005146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows √ó 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  platform country  region language category     hashtag  \\\n",
       "0   TikTok      Jp    Asia       ja   Gaming  #Lifestyle   \n",
       "1   TikTok      Se  Europe       sv     Food     #Sports   \n",
       "2   TikTok      Za  Africa       en      Art    #Workout   \n",
       "\n",
       "              title_keywords    author_handle sound_type  music_track  ...  \\\n",
       "0    Night Routine ‚Äî College       NextVision   trending    8bit loop  ...   \n",
       "1  Morning Routine ‚Äî College  DailyVlogsDiego   trending  Street vibe  ...   \n",
       "2    Night Routine ‚Äî College        BeyondHub   licensed  Gallery pad  ...   \n",
       "\n",
       "   traffic_source  is_weekend                            row_id  \\\n",
       "0        External           1  2e681528d17a1fe1986857942536ec27   \n",
       "1          Search           0  2e35fa0b2978b9cae635839c1d4e9e74   \n",
       "2        External           1  0d88a011235a82244995ef52961f9502   \n",
       "\n",
       "   engagement_total  like_rate  dislike_rate  engagement_per_1k  \\\n",
       "0             30317   0.086159      0.004004            120.069   \n",
       "1             30577   0.085298      0.002421            113.005   \n",
       "2               503   0.049154      0.001625             68.111   \n",
       "\n",
       "   engagement_like_rate engagement_comment_rate engagement_share_rate  \n",
       "0              0.086159                0.012555              0.007830  \n",
       "1              0.085298                0.007850              0.007791  \n",
       "2              0.049154                0.004469              0.005146  \n",
       "\n",
       "[3 rows x 58 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Path to your CSV file (adjust if needed)\n",
    "csv_path = 'data/raw/youtube_shorts_tiktok_trends_2025.csv'\n",
    "\n",
    "print(f\"Loading TINY dataset from: {csv_path}\")\n",
    "print(\"(Only 100 rows for dry run)\")\n",
    "\n",
    "try:\n",
    "    # Check if file exists\n",
    "    if not Path(csv_path).exists():\n",
    "        print(f\"‚ùå File not found: {csv_path}\")\n",
    "        print(f\"Current directory: {os.getcwd()}\")\n",
    "        print(f\"Looking for: {Path(csv_path).absolute()}\")\n",
    "        raise FileNotFoundError(f\"Dataset not found at {csv_path}\")\n",
    "    \n",
    "    df_raw = pd.read_csv(csv_path, nrows=100)  # Only 100 rows!\n",
    "    print(f\"‚úÖ Loaded {len(df_raw)} rows\")\n",
    "    print(f\"\\nShape: {df_raw.shape}\")\n",
    "    print(f\"\\nFirst 10 columns: {list(df_raw.columns[:10])}\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(\"\\nFirst 3 rows:\")\n",
    "    display(df_raw.head(3))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå FAILED TO LOAD DATA: {e}\")\n",
    "    print(\"\\nMake sure:\")\n",
    "    print(\"1. CSV file exists at data/raw/youtube_shorts_tiktok_trends_2025.csv\")\n",
    "    print(\"2. File is not corrupted\")\n",
    "    print(\"3. You're in the project root directory\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Test Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:data.dataset_adapter:======================================================================\n",
      "INFO:data.dataset_adapter:Preparing dataset for training\n",
      "INFO:data.dataset_adapter:======================================================================\n",
      "INFO:data.dataset_adapter:Adapting dataset columns to expected format...\n",
      "INFO:data.dataset_adapter:Renamed columns: ['row_id', 'publish_date_approx', 'hashtag']\n",
      "INFO:data.dataset_adapter:‚úÖ All required columns present\n",
      "INFO:data.dataset_adapter:engagement_velocity statistics:\n",
      "INFO:data.dataset_adapter:  Mean: 13284.95\n",
      "INFO:data.dataset_adapter:  Std: 19031.46\n",
      "INFO:data.dataset_adapter:  Min: 338.14\n",
      "INFO:data.dataset_adapter:  Max: 80313.67\n",
      "INFO:data.dataset_adapter:Created viral labels: 4/19 viral (21.1%)\n",
      "INFO:data.dataset_adapter:Viral threshold (engagement_velocity): 22970.74\n",
      "INFO:data.dataset_adapter:Found 18 scalar features in dataset\n",
      "INFO:data.dataset_adapter:Available scalar features (18):\n",
      "INFO:data.dataset_adapter:  - views\n",
      "INFO:data.dataset_adapter:  - likes\n",
      "INFO:data.dataset_adapter:  - comments\n",
      "INFO:data.dataset_adapter:  - shares\n",
      "INFO:data.dataset_adapter:  - saves\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data preparation...\n",
      "Filtering data...\n",
      "  After language filter: 19 rows\n",
      "  After dropping nulls: 19 rows (removed 0)\n",
      "\n",
      "Preparing dataset with adapter...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:data.dataset_adapter:  - engagement_rate\n",
      "INFO:data.dataset_adapter:  - completion_rate\n",
      "INFO:data.dataset_adapter:  - like_rate\n",
      "INFO:data.dataset_adapter:  - comment_ratio\n",
      "INFO:data.dataset_adapter:  - share_rate\n",
      "INFO:data.dataset_adapter:  - save_rate\n",
      "INFO:data.dataset_adapter:  - upload_hour\n",
      "INFO:data.dataset_adapter:  - publish_dayofweek\n",
      "INFO:data.dataset_adapter:  - is_weekend\n",
      "INFO:data.dataset_adapter:  - duration_sec\n",
      "INFO:data.dataset_adapter:  - title_length\n",
      "INFO:data.dataset_adapter:  - has_emoji\n",
      "INFO:data.dataset_adapter:  - creator_avg_views\n",
      "INFO:data.dataset_adapter:======================================================================\n",
      "INFO:data.dataset_adapter:Dataset preparation complete!\n",
      "INFO:data.dataset_adapter:Shape: (19, 59)\n",
      "INFO:data.dataset_adapter:Text column: title\n",
      "INFO:data.dataset_adapter:Has viral labels: True\n",
      "INFO:data.dataset_adapter:Has engagement_velocity: True\n",
      "INFO:data.dataset_adapter:======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset prepared: (19, 59)\n",
      "\n",
      "Dataset summary:\n",
      "  Total videos: 19\n",
      "  Viral rate: 21.1%\n",
      "\n",
      "Prepared columns (59):\n",
      "['platform', 'country', 'region', 'language', 'category', 'primary_hashtag', 'title_keywords', 'author_handle', 'sound_type', 'music_track', 'week_of_year', 'duration_sec', 'views', 'likes', 'comments', 'shares', 'saves', 'engagement_rate', 'trend_label', 'source_hint', 'notes', 'device_type', 'upload_hour', 'genre', 'trend_duration_days', 'trend_type', 'engagement_velocity', 'dislikes', 'comment_ratio', 'share_rate', 'save_rate', 'like_dislike_ratio', 'publish_dayofweek', 'publish_period', 'event_season', 'tags', 'sample_comments', 'creator_avg_views', 'creator_tier', 'season', 'upload_date', 'year_month', 'title', 'title_length', 'has_emoji', 'avg_watch_time_sec', 'completion_rate', 'device_brand', 'traffic_source', 'is_weekend', 'video_id', 'engagement_total', 'like_rate', 'dislike_rate', 'engagement_per_1k', 'engagement_like_rate', 'engagement_comment_rate', 'engagement_share_rate', 'is_viral']\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing data preparation...\")\n",
    "\n",
    "try:\n",
    "    # Basic filtering\n",
    "    print(\"Filtering data...\")\n",
    "    if 'language' in df_raw.columns:\n",
    "        df_filtered = df_raw[df_raw['language'] == 'en'].copy()\n",
    "        print(f\"  After language filter: {len(df_filtered)} rows\")\n",
    "    else:\n",
    "        df_filtered = df_raw.copy()\n",
    "        print(f\"  No language column, using all rows\")\n",
    "    \n",
    "    # Drop missing values\n",
    "    critical_cols = ['row_id', 'title', 'views', 'likes']\n",
    "    before_count = len(df_filtered)\n",
    "    df_filtered = df_filtered.dropna(subset=critical_cols)\n",
    "    print(f\"  After dropping nulls: {len(df_filtered)} rows (removed {before_count - len(df_filtered)})\")\n",
    "    \n",
    "    # Prepare with adapter\n",
    "    print(\"\\nPreparing dataset with adapter...\")\n",
    "    df_prepared = prepare_dataset_for_training(\n",
    "        df_filtered,\n",
    "        text_column='title',\n",
    "        create_viral_labels=True,\n",
    "        viral_threshold_percentile=80.0\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Dataset prepared: {df_prepared.shape}\")\n",
    "    \n",
    "    # Get summary\n",
    "    summary = get_dataset_summary(df_prepared)\n",
    "    print(f\"\\nDataset summary:\")\n",
    "    print(f\"  Total videos: {summary.get('total_videos', len(df_prepared))}\")\n",
    "    print(f\"  Viral rate: {summary.get('viral_percentage', 0):.1f}%\")\n",
    "    \n",
    "    # Show prepared columns\n",
    "    print(f\"\\nPrepared columns ({len(df_prepared.columns)}):\")\n",
    "    print(list(df_prepared.columns))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå DATA PREPARATION FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test Train/Val/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data splits...\n",
      "‚ùå SPLIT FAILED: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/yg/1222qhpn71sclw0r8zspmz2h0000gn/T/ipykernel_16313/1566988088.py\", line 4, in <module>\n",
      "    train_df, val_df, test_df = create_train_val_test_split(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/cheneyyoon/Desktop/U of T/APS360/Miles/src/data/dataset.py\", line 305, in create_train_val_test_split\n",
      "    val_df, test_df = train_test_split(\n",
      "                      ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/cheneyyoon/Desktop/U of T/APS360/Miles/venv/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 218, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/cheneyyoon/Desktop/U of T/APS360/Miles/venv/lib/python3.12/site-packages/sklearn/model_selection/_split.py\", line 2940, in train_test_split\n",
      "    train, test = next(cv.split(X=arrays[0], y=stratify))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/cheneyyoon/Desktop/U of T/APS360/Miles/venv/lib/python3.12/site-packages/sklearn/model_selection/_split.py\", line 1927, in split\n",
      "    for train, test in self._iter_indices(X, y, groups):\n",
      "  File \"/Users/cheneyyoon/Desktop/U of T/APS360/Miles/venv/lib/python3.12/site-packages/sklearn/model_selection/_split.py\", line 2342, in _iter_indices\n",
      "    raise ValueError(\n",
      "ValueError: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTesting data splits...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     train_df, val_df, test_df = \u001b[43mcreate_train_val_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdf_prepared\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mval_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstratify_column\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mis_viral\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Splits created:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Train: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m samples (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[32m100\u001b[39m*train_df[\u001b[33m'\u001b[39m\u001b[33mis_viral\u001b[39m\u001b[33m'\u001b[39m].mean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m% viral)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/U of T/APS360/Miles/src/data/dataset.py:305\u001b[39m, in \u001b[36mcreate_train_val_test_split\u001b[39m\u001b[34m(df, train_ratio, val_ratio, test_ratio, stratify_column, random_seed)\u001b[39m\n\u001b[32m    303\u001b[39m     \u001b[38;5;66;03m# Second split: val + test\u001b[39;00m\n\u001b[32m    304\u001b[39m     val_size_adjusted = val_ratio / (val_ratio + test_ratio)\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m     val_df, test_df = \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemp_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_size_adjusted\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstratify\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemp_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstratify_column\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandom_seed\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    312\u001b[39m     \u001b[38;5;66;03m# Random split\u001b[39;00m\n\u001b[32m    313\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/U of T/APS360/Miles/venv/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/U of T/APS360/Miles/venv/lib/python3.12/site-packages/sklearn/model_selection/_split.py:2940\u001b[39m, in \u001b[36mtrain_test_split\u001b[39m\u001b[34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[39m\n\u001b[32m   2936\u001b[39m         CVClass = ShuffleSplit\n\u001b[32m   2938\u001b[39m     cv = CVClass(test_size=n_test, train_size=n_train, random_state=random_state)\n\u001b[32m-> \u001b[39m\u001b[32m2940\u001b[39m     train, test = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstratify\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2942\u001b[39m train, test = ensure_common_namespace_device(arrays[\u001b[32m0\u001b[39m], train, test)\n\u001b[32m   2944\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[32m   2945\u001b[39m     chain.from_iterable(\n\u001b[32m   2946\u001b[39m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[32m   2947\u001b[39m     )\n\u001b[32m   2948\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/U of T/APS360/Miles/venv/lib/python3.12/site-packages/sklearn/model_selection/_split.py:1927\u001b[39m, in \u001b[36mBaseShuffleSplit.split\u001b[39m\u001b[34m(self, X, y, groups)\u001b[39m\n\u001b[32m   1897\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[32m   1898\u001b[39m \n\u001b[32m   1899\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1924\u001b[39m \u001b[33;03mto an integer.\u001b[39;00m\n\u001b[32m   1925\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1926\u001b[39m X, y, groups = indexable(X, y, groups)\n\u001b[32m-> \u001b[39m\u001b[32m1927\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iter_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1928\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/U of T/APS360/Miles/venv/lib/python3.12/site-packages/sklearn/model_selection/_split.py:2342\u001b[39m, in \u001b[36mStratifiedShuffleSplit._iter_indices\u001b[39m\u001b[34m(self, X, y, groups)\u001b[39m\n\u001b[32m   2340\u001b[39m class_counts = np.bincount(y_indices)\n\u001b[32m   2341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np.min(class_counts) < \u001b[32m2\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2342\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2343\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe least populated class in y has only 1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2344\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m member, which is too few. The minimum\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2345\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m number of groups for any class cannot\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2346\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m be less than 2.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2347\u001b[39m     )\n\u001b[32m   2349\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_train < n_classes:\n\u001b[32m   2350\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2351\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe train_size = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m should be greater or \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2352\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mequal to the number of classes = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m\"\u001b[39m % (n_train, n_classes)\n\u001b[32m   2353\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2."
     ]
    }
   ],
   "source": [
    "print(\"Testing data splits...\")\n",
    "\n",
    "try:\n",
    "    train_df, val_df, test_df = create_train_val_test_split(\n",
    "        df_prepared,\n",
    "        train_ratio=0.7,\n",
    "        val_ratio=0.15,\n",
    "        test_ratio=0.15,\n",
    "        stratify_column='is_viral',\n",
    "        random_seed=42\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Splits created:\")\n",
    "    print(f\"  Train: {len(train_df)} samples ({100*train_df['is_viral'].mean():.1f}% viral)\")\n",
    "    print(f\"  Val:   {len(val_df)} samples ({100*val_df['is_viral'].mean():.1f}% viral)\")\n",
    "    print(f\"  Test:  {len(test_df)} samples ({100*test_df['is_viral'].mean():.1f}% viral)\")\n",
    "    print(f\"\\n  Total: {len(train_df) + len(val_df) + len(test_df)} (original: {len(df_prepared)})\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå SPLIT FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test Model Initialization (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing model initialization...\")\n",
    "print(\"(This will download BERT weights if not cached - may take 1-2 minutes)\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    # Get scalar features\n",
    "    scalar_features = get_available_scalar_features(df_prepared)\n",
    "    print(f\"Using {len(scalar_features)} scalar features:\")\n",
    "    for feat in scalar_features:\n",
    "        print(f\"  - {feat}\")\n",
    "    \n",
    "    # Initialize model on CPU\n",
    "    print(\"\\nInitializing model...\")\n",
    "    model = MultimodalViralityPredictor(\n",
    "        num_scalar_features=len(scalar_features),\n",
    "        freeze_encoders=True,\n",
    "        fusion_hidden_dims=[1024, 256],\n",
    "        dropout_rates=[0.3, 0.2],\n",
    "        use_text=True,\n",
    "        use_vision=False  # No images in this dataset\n",
    "    )\n",
    "    \n",
    "    params = model.count_parameters()\n",
    "    print(f\"\\n‚úÖ Model initialized on CPU\")\n",
    "    print(f\"  Total params: {params['total']:,}\")\n",
    "    print(f\"  Trainable: {params['trainable']:,}\")\n",
    "    print(f\"  Frozen: {params['frozen']:,}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå MODEL INITIALIZATION FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test DataLoader Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing DataLoader creation...\")\n",
    "\n",
    "try:\n",
    "    from data.dataset import ViralShortsDataset, collate_multimodal_batch\n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    # Create small dataset\n",
    "    print(\"Creating dataset...\")\n",
    "    train_dataset = ViralShortsDataset(\n",
    "        train_df,\n",
    "        text_column='title',\n",
    "        scalar_columns=scalar_features,\n",
    "        label_column='is_viral',\n",
    "        velocity_column='engagement_velocity',\n",
    "        text_max_length=128,\n",
    "        use_images=False,\n",
    "        augment_images=False\n",
    "    )\n",
    "    \n",
    "    print(f\"  Dataset size: {len(train_dataset)} samples\")\n",
    "    \n",
    "    # Create DataLoader with small batch\n",
    "    print(\"\\nCreating DataLoader...\")\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=4,  # Small batch for testing\n",
    "        shuffle=True, \n",
    "        num_workers=0,  # No multiprocessing for local testing\n",
    "        collate_fn=collate_multimodal_batch\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ DataLoader created: {len(train_loader)} batches\")\n",
    "    \n",
    "    # Test one batch\n",
    "    print(\"\\nLoading test batch...\")\n",
    "    batch = next(iter(train_loader))\n",
    "    print(f\"‚úÖ Batch loaded successfully\")\n",
    "    print(f\"  Text shape: {batch['text']['input_ids'].shape}\")\n",
    "    print(f\"  Scalars shape: {batch['scalars'].shape}\")\n",
    "    print(f\"  Labels shape: {batch['label'].shape}\")\n",
    "    print(f\"  Velocity shape: {batch['velocity'].shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå DATALOADER CREATION FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing forward pass...\")\n",
    "\n",
    "try:\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"Running inference...\")\n",
    "    with torch.no_grad():\n",
    "        cls_logits, reg_output = model(\n",
    "            text_input=batch['text'],\n",
    "            image_input=None,\n",
    "            scalar_features=batch['scalars']\n",
    "        )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Forward pass successful!\")\n",
    "    print(f\"  Classification logits: {cls_logits.shape}\")\n",
    "    print(f\"  Regression output: {reg_output.shape}\")\n",
    "    \n",
    "    # Show sample predictions\n",
    "    probs = torch.softmax(cls_logits, dim=1)\n",
    "    print(f\"\\n  Sample predictions (viral probability):\")\n",
    "    for i in range(min(3, len(probs))):\n",
    "        print(f\"    Sample {i+1}: {probs[i, 1].item():.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå FORWARD PASS FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Test One Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing one training step (backward pass)...\")\n",
    "\n",
    "try:\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    cls_criterion = nn.CrossEntropyLoss()\n",
    "    reg_criterion = nn.MSELoss()\n",
    "    \n",
    "    # One training step\n",
    "    print(\"Running training step...\")\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    cls_logits, reg_output = model(\n",
    "        text_input=batch['text'],\n",
    "        image_input=None,\n",
    "        scalar_features=batch['scalars']\n",
    "    )\n",
    "    \n",
    "    cls_loss = cls_criterion(cls_logits, batch['label'])\n",
    "    reg_loss = reg_criterion(reg_output.squeeze(), batch['velocity'])\n",
    "    \n",
    "    total_loss = 0.7 * cls_loss + 0.3 * reg_loss\n",
    "    \n",
    "    print(\"Running backward pass...\")\n",
    "    total_loss.backward()\n",
    "    \n",
    "    print(\"Updating parameters...\")\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Training step successful!\")\n",
    "    print(f\"  Total loss: {total_loss.item():.4f}\")\n",
    "    print(f\"  Classification loss: {cls_loss.item():.4f}\")\n",
    "    print(f\"  Regression loss: {reg_loss.item():.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå TRAINING STEP FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Test Configuration Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing configuration loading...\")\n",
    "\n",
    "try:\n",
    "    config = load_config('src/configs/training_config.yaml')\n",
    "    \n",
    "    print(f\"\\n‚úÖ Config loaded successfully\")\n",
    "    print(f\"\\nConfiguration:\")\n",
    "    print(f\"  Model name: {config['model']['name']}\")\n",
    "    print(f\"  Epochs: {config['training']['epochs']}\")\n",
    "    print(f\"  Batch size: {config['data']['batch_size']}\")\n",
    "    print(f\"  Learning rate: {config['training']['learning_rate']}\")\n",
    "    print(f\"  Device: {config['hardware']['device']}\")\n",
    "    print(f\"  Mixed precision: {config['hardware']['mixed_precision']}\")\n",
    "    \n",
    "    # Validate required keys\n",
    "    required_keys = ['model', 'data', 'training', 'hardware']\n",
    "    for key in required_keys:\n",
    "        assert key in config, f\"Missing required key: {key}\"\n",
    "    \n",
    "    print(f\"\\n  All required config keys present ‚úì\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå CONFIG LOADING FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"LOCAL DRY RUN VALIDATION - FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"‚úÖ All tests passed! Your pipeline is ready.\")\n",
    "print()\n",
    "print(\"What was tested:\")\n",
    "print(\"  ‚úì All module imports\")\n",
    "print(\"  ‚úì Data loading and preprocessing\")\n",
    "print(\"  ‚úì Train/val/test splitting\")\n",
    "print(\"  ‚úì Model initialization (BERT + MLP fusion)\")\n",
    "print(\"  ‚úì DataLoader creation\")\n",
    "print(\"  ‚úì Forward pass (inference)\")\n",
    "print(\"  ‚úì Backward pass (training step)\")\n",
    "print(\"  ‚úì Configuration loading\")\n",
    "print()\n",
    "print(\"Environment:\")\n",
    "print(f\"  Device: CPU\")\n",
    "print(f\"  Test samples: {len(df_prepared)}\")\n",
    "print(f\"  Model params: {params['total']:,}\")\n",
    "print()\n",
    "print(\"Next steps:\")\n",
    "print(\"  1. Your code is validated and ready!\")\n",
    "print(\"  2. Upload to Google Drive at MyDrive/Miles/\")\n",
    "print(\"  3. Open phase1_training_colab.ipynb in Colab\")\n",
    "print(\"  4. Select A100 GPU runtime\")\n",
    "print(\"  5. Run all cells with confidence! üöÄ\")\n",
    "print()\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
